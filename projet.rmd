---
title: "Projet"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Projet d'Analyse et de Fouille de Données Massive

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FactoMineR)
```

doc si perdu : <http://rmarkdown.rstudio.com>

## Introduction :
Le sujet nous demande de faire blabla

## Présentation de nos données :

Nous avons extrait nos donnée de https://www.kaggle.com/datasnaek/youtube-new.
Nous avons décidé d'extraire les attributs suivant :

|                 Nom                |   Attribut  |
|:----------------------------------:|:-----------:|
|           Taux LOWER Case          | quantitatif |
|           Taux UPPER Case          | quantitatif |
|               Nb tags              | quantitatif |
|               Nb vues              | quantitatif |
|           Nb commentaires          | quantitatif |
|              Nb likes              | quantitatif |
|             Nb dislikes            | quantitatif |
| Taille de la  description (en mot) | quantitatif |
|              Catégorie             |  qualitatif |
| Jour de la semaine                 |  qualitatif |
|        Moment de la  journée       |  qualitatif |
|    Nombre de lien en description   | quantitatif |

=====
AJOUTER COMMENTAIRE SUR CE QU'ON ATTENDS DE CHAQUE VARIABLE
=====

Vous pouvez retrouver le code commenté pour l'extraction dans le script 'processData.py'.

## Analyse des données
Dans cette partie, nous allons analyser les données que nous avons extrait et qui se trouvent dans le fichier "youtubeTrends.csv". Ce fichier contient 40703 individus, ce qui est beaucoups trop ! Nous voulons avoir uniquement une centaine d'invidus, nous choisisons donc une centaine d'individu aléatoirement. 

Code pour charger les 100 individus aléatoire :
```{r}
data = read.csv("./DATA/youtubeTrends.csv")
data = data[sample(nrow(data), 100),]
```

Code pour récupérer les 100 individus que nous avons utilisé et sauvegardé :
```{r}
setwd("CHEMIN VERS LA RACINE DU DOSSIER CONTENANT LE RDATA")
load('./random_data.rdata')
```


### ACP
On récupère les données :




La première analyse que nous allons essayé sera une ACP, pour ce faire, il faut que l'on prépare nos données en retirant les données qualitatives que nous avons.
Ensuite On execute l'ACP sur nos données.
```{r}
# On retire les données qualitatives
data_ACP = data
data_ACP$category <- NULL
data_ACP$momentOfDay <- NULL
data_ACP$day <- NULL
data_ACP$index <- NULL

# On retire les données considéré comme faiblement corellé après les premiers tests 
data_ACP$nbTags <- NULL
data_ACP$nbWords <- NULL
data_ACP$nbLinks <- NULL

res.pca = PCA(data_ACP, scale.unit=TRUE, ncp=6, graph=T)
```

Sur la figure on peut voir que l'on représente bien les données X% sur l'axe 1 et Y% sur l'axe 2.
On voit que le nombre de like et dislike sont corrélé, après analyse, on se rend compte que ces deux variables ne capturent pas vraiment ce que l'on voulait. En effet

```{r}
# On retire les données qualitatives
data_ACP = data
data_ACP$category <- NULL
data_ACP$momentOfDay <- NULL
data_ACP$day <- NULL
data_ACP$index <- NULL

# On retire les données faiblement corellé
data_ACP$nbTags <- NULL
data_ACP$nbWords <- NULL
data_ACP$nbLinks <- NULL

# Ajout du ratio de like et dislike
data_ACP$ratioLikes <- data_ACP$nbLikes/(data_ACP$nbLikes + data_ACP$nbDislikes)
data_ACP$ratioDislikes <- data_ACP$nbDislikes/(data_ACP$nbLikes + data_ACP$nbDislikes)

data_ACP$nbLikes <- NULL
data_ACP$nbDislikes <- NULL

res.pca = PCA(data_ACP, scale.unit=TRUE, ncp=6, graph=T) 
```

# ACM
```{r}
data_ACM = data[1:50,]
i=0
while(i<ncol(data_ACM)){
  i=i+1
  data_ACM[,i]=as.factor(data_ACM[,i])
}
data_ACM$nbTags <- NULL
data_ACM$nbWords <- NULL
data_ACM$nbLinks <- NULL
res.mca = MCA(data_ACM, quali.sup = c(7,8,10), graph = TRUE)
plot.MCA(res.mca, invisible=c("var"), cex=0.35)
```

# AFC
```{r}
data_AFC = data
res.afc = CA(data_AFC[, c(2:5 ,12:13)])
```


