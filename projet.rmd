---
title: "Projet"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# Projet d'Analyse et de Fouille de Données Massive

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(FactoMineR)
```

&ensp; doc si perdu : <http://rmarkdown.rstudio.com>

## Introduction :
Le sujet nous demande de faire blabla

## Présentation de nos données :

Nous avons extrait nos donnée de https://www.kaggle.com/datasnaek/youtube-new.
Nous avons décidé d'extraire les attributs suivant :

|                 Nom                |   Attribut  |
|:----------------------------------:|:-----------:|
|           Taux LOWER Case          | quantitatif |
|           Taux UPPER Case          | quantitatif |
|               Nb tags              | quantitatif |
|               Nb vues              | quantitatif |
|           Nb commentaires          | quantitatif |
|              Nb likes              | quantitatif |
|             Nb dislikes            | quantitatif |
| Taille de la  description (en mot) | quantitatif |
|              Catégorie             |  qualitatif |
| Jour de la semaine                 |  qualitatif |
|        Moment de la  journée       |  qualitatif |
|    Nombre de lien en description   | quantitatif |

=====
AJOUTER COMMENTAIRE SUR CE QU'ON ATTENDS DE CHAQUE VARIABLE
=====

Vous pouvez retrouver le code commenté pour l'extraction dans le script 'processData.py'. Pour l'utiliser veuillez télécharger les données dans un fichier 'DATA' à la racine du projet. Vous n'avez besoin de ne télécharger que "FR_category_id.json" et "FRvideos.csv".

## Analyse des données
Dans cette partie, nous allons analyser les données que nous avons extrait et qui se trouvent dans le fichier "youtubeTrends.csv". Ce fichier contient 40703 individus, ce qui est beaucoups trop ! Nous voulons avoir uniquement une centaine d'invidus, nous choisisons donc une centaine d'individu aléatoirement. 

Code pour récupérer les 100 individus que nous avons utilisé et sauvegardé :
```{r}
setwd("/home/jyra/Documents/Enssat/3A/AFDM/DataMiningProject")
load('./random_data.rdata')
```


### ACP
On récupère les données :


La première analyse que nous allons essayé sera une ACP, pour ce faire, il faut que l'on prépare nos données en retirant les données qualitatives que nous avons.
Ensuite On execute l'ACP sur nos données.
```{r}
# On retire les données qualitatives
data_ACP = data
data_ACP$category <- NULL
data_ACP$momentOfDay <- NULL
data_ACP$day <- NULL
data_ACP$index <- NULL

# On retire les données considéré comme faiblement corellé après les premiers tests 
data_ACP$nbTags <- NULL
data_ACP$nbWords <- NULL
data_ACP$nbLinks <- NULL

res.pca = PCA(data_ACP, scale.unit=TRUE, ncp=6, graph=T)
```

Sur la figure on peut voir que l'on représente bien les données X% sur l'axe 1 et Y% sur l'axe 2.
On voit que le nombre de like et dislike sont corrélé, après analyse, on se rend compte que ces deux variables ne capturent pas vraiment ce que l'on voulait. En effet

```{r}
# On retire les données qualitatives
data_ACP = data
data_ACP$category <- NULL
data_ACP$momentOfDay <- NULL
data_ACP$day <- NULL
data_ACP$index <- NULL

# On retire les données faiblement corellé
data_ACP$nbTags <- NULL
data_ACP$nbWords <- NULL
data_ACP$nbLinks <- NULL

# Ajout du ratio de like et dislike
data_ACP$ratioLikes <- data_ACP$nbLikes/(data_ACP$nbLikes + data_ACP$nbDislikes)
data_ACP$ratioDislikes <- data_ACP$nbDislikes/(data_ACP$nbLikes + data_ACP$nbDislikes)

data_ACP$nbLikes <- NULL
data_ACP$nbDislikes <- NULL

res.pca = PCA(data_ACP, scale.unit=TRUE, ncp=6, graph=T) 
```

## ACM

Dans cette partie, nous allons réaliser une ACM sur 3 attributs qualitatif : le moment de la semaine où la vidéo est publiée, le moment de la journée et la catégorie de la vidéo.

Pour commencer, on prépare nos données, on enlève la moitié des données pour faire l'ACM car sans ça, on se retrouve avec tellement de point que l'on ne peut rien lire. Ensuite, on doit effectuer une petite opération pour transformer nos données en factor (un type spécifique à R). Cette opération est necessaire car sans cela, nous avions une erreure. Enfin, on enlève tous les attributs quantitatif sauf : le nombre de like, dislike et de commentaire. On garde ces attributs car on veut regrouper les attributs quantitatifs en fonctions de leurs succés qui se représente à l'aide des 3 variables quantitatives ci-dessus.

```{r}
data_ACM = data[1:50,] 
i=0
while(i<ncol(data_ACM)){
  i=i+1
  data_ACM[,i]=as.factor(data_ACM[,i])
}
data_ACM$index <- NULL
data_ACM$nbTags <- NULL
data_ACM$nbWords <- NULL
data_ACM$nbLinks <- NULL
data_ACM$X.lowerCase <- NULL
data_ACM$X.upperCase <- NULL

```

### Moment de la semaine

Nous nous interressons au moment de la journée où la vidéo à été publiée. On s'attends éventuellement à voir regroupé ensemble des jours de la semaine en fonction de si ils sont en début milieu et fin de semaine. 

```{r}
res.mca_1 = MCA(data_ACM, quali.sup = c(4), graph = FALSE)
plot.MCA(res.mca_1, invisible=c("var"), cex=0.75)
```

Les résultats obtenues sont plutôt satisfaisant, même si l'ont représente peux les données (5.31%), on observe que certains jours sont reliées comme le Jeudi et Mercredi. Cela semble logique ces deux jours doivent posséder la même visibilité car ils se trouvent en milieu de semaine. Le Vendredi se retrouve un peu à part, on peut imaginer qu'elles s'adressent à un publique adulte qui regarderait les vidéos après le travail. Résultat plus étonnant, on voit que le Samedi et Lundi sont liées ainsi que le Mardi et Dimanche. 

```{r}
res.mca_2 = MCA(data_ACM, quali.sup = c(5), graph = FALSE)
plot.MCA(res.mca_2, invisible=c("var"), cex=0.75)
```

```{r}
res.mca_3 = MCA(data_ACM, quali.sup = c(7), graph = FALSE)
plot.MCA(res.mca_3, invisible=c("var"), cex=0.75)
```





# AFC
```{r}
data_AFC = data
res.afc = CA(data_AFC[, c(2:5 ,12:13)])
```


